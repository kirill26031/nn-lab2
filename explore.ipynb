{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import dataset, PlantOrgansDataset\n",
    "from preprocessing import preprocess_image_and_mask\n",
    "import torchvision.transforms.v2 as T\n",
    "import torch\n",
    "import numpy as np\n",
    "from alexnet import patch_index_to_position, image_to_patches, MyTransform\n",
    "from train import device, pixel_validate, patch_loss, patch_validate, evaluate, fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "commonTransform = T.Compose([\n",
    "        T.Resize(size=(2048, 2048)),\n",
    "        T.ToImage()\n",
    "        \n",
    "        # T.RandomHorizontalFlip(p=0.5),\n",
    "        # T.RandomVerticalFlip(p=0.5),\n",
    "        # T.RandomRotation(degrees=45)\n",
    "    ])\n",
    "imagesTransform = T.Compose([\n",
    "    T.ToDtype(torch.float32, scale=False),\n",
    "    # T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    MyTransform(64),\n",
    "    T.Resize((224, 224)),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "masksTransform = T.Compose([\n",
    "    T.ToDtype(torch.int8, scale=False),\n",
    "    # T.Normalize(mean=[0.0014], std=[0.0031]),\n",
    "    MyTransform(64),\n",
    "    # T.Resize((224, 224))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validation_data = dataset['train'].train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = PlantOrgansDataset(train_validation_data['train'], commonTransform, imagesTransform, masksTransform)\n",
    "validation_dataset = PlantOrgansDataset(train_validation_data['test'], commonTransform, imagesTransform, masksTransform)\n",
    "test_dataset = PlantOrgansDataset(dataset['validation'], commonTransform, imagesTransform, masksTransform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4, dtype=torch.int8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = T.Compose([\n",
    "    T.ToImage(),\n",
    "    T.ToDtype(dtype=torch.int8, scale=False)\n",
    "])\n",
    "tensor = tr(train_validation_data['train'][0]['label'])\n",
    "tensor.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_weights = torch.tensor([\n",
    "        4.8033e-04,\n",
    "        6.4129e-03,\n",
    "        3.9272e-03,\n",
    "        9.7140e-01,\n",
    "        1.7778e-02], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset:  4596\n",
      "validation_dataset:  1149\n",
      "test_dataset:  1437\n"
     ]
    }
   ],
   "source": [
    "print(\"train_dataset: \", len(train_dataset))\n",
    "print(\"validation_dataset: \", len(validation_dataset))\n",
    "print(\"test_dataset: \", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedDataLoader:\n",
    "    def __init__(self, loader, func):\n",
    "        self.loader = loader\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.loader)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in iter(self.loader):\n",
    "            batch_cuda = []\n",
    "            for X, y in batch:\n",
    "                batch_cuda.append(self.func(X, y))\n",
    "            yield batch_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(X: torch.Tensor, y: torch.Tensor):\n",
    "    return X.to(device, dtype=torch.float32), y.to(device, dtype=torch.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    batchs_amount = len(batch)\n",
    "    current_images = []\n",
    "    current_masks = []\n",
    "    current_length = 0\n",
    "    i = 0\n",
    "    while i < batchs_amount or current_length >= batch_size:\n",
    "        if current_length == batch_size:\n",
    "            if len(current_images) == 1:\n",
    "                result_images = current_images[0]\n",
    "                result_masks = current_masks[0]\n",
    "            else:\n",
    "                result_images = torch.concatenate(current_images)\n",
    "                result_masks = torch.concatenate(current_masks)\n",
    "            current_images = []\n",
    "            current_masks = []\n",
    "            current_length = 0\n",
    "            yield result_images, result_masks\n",
    "        elif current_length > batch_size:\n",
    "            concatenated_images = torch.concatenate(current_images)\n",
    "            concatenated_masks = torch.concatenate(current_masks)\n",
    "            images_split = torch.split(concatenated_images, batch_size, dim=0)\n",
    "            masks_split = torch.split(concatenated_masks, batch_size, dim=0)\n",
    "            current_images = [images_split[len(images_split) - 1]]\n",
    "            current_masks = [masks_split[len(masks_split) - 1]]\n",
    "            current_length = len(current_images[0])\n",
    "            for j in range(len(images_split) - 1):\n",
    "                yield images_split[j], masks_split[j]\n",
    "        else:  \n",
    "            images, masks = batch[i]\n",
    "            i += 1\n",
    "            current_length += len(images)\n",
    "            current_images.append(images)\n",
    "            current_masks.append(masks)\n",
    "    if current_length > 0:\n",
    "        concatenated_images = torch.concatenate(current_images)\n",
    "        concatenated_masks = torch.concatenate(current_masks)\n",
    "        yield concatenated_images, concatenated_masks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = WrappedDataLoader(\n",
    "    DataLoader(train_dataset, batch_size=1, shuffle=False, collate_fn=custom_collate_fn, \n",
    "               pin_memory=False, pin_memory_device=[device]), to_device)\n",
    "valid_loader = WrappedDataLoader(\n",
    "    DataLoader(validation_dataset, batch_size=1, shuffle=False, collate_fn=custom_collate_fn,\n",
    "               pin_memory=True, pin_memory_device=[device]), to_device)\n",
    "test_loader = WrappedDataLoader(\n",
    "    DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=custom_collate_fn,\n",
    "               pin_memory=True, pin_memory_device=[device]), to_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\pc/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=False).to(device)\n",
    "\n",
    "model.classifier[6] = torch.nn.Linear(in_features=4096, out_features=5, bias=True).to(device)\n",
    "\n",
    "# model = AlexNet([11, 5, 3, 3, 3], [96, 256, 384, 384, 256], [0, 2, 1, 1, 1], [4096, 4096], 4, 224).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "to_image = T.ToPILImage()\n",
    "mask_to_image = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.ToDtype(torch.float16),\n",
    "    T.Normalize(mean=[0.0014], std=[0.0031]),\n",
    "    T.ToPILImage(),\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in valid_loader:\n",
    "#     # print(len(batch))\n",
    "#     for X, y in batch:\n",
    "#         print(X.shape)\n",
    "#         predicted = model(X)\n",
    "#         loss_, correct, _ = patch_validate(model, torch.nn.CrossEntropyLoss(weight=cross_entropy_weights), X, y)\n",
    "#         print(loss_, correct)\n",
    "#         loss_, correct, _ = pixel_validate(model, torch.nn.CrossEntropyLoss(weight=cross_entropy_weights), X, y)\n",
    "#         print(loss_, correct)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts = torch.zeros(size=(5, 1), dtype=torch.int64).to(device)\n",
    "# for batch in valid_loader:\n",
    "#     for X, y in batch:\n",
    "#         current_counts = y.unique(return_counts=True)\n",
    "#         for i in current_counts[0]:\n",
    "#             counts[current_counts[0][i]] += current_counts[1][i]\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts_sum = torch.sum(counts).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = torch.div(counts_sum, torch.multiply(counts, 2549.76513671875))\n",
    "\n",
    "# print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 19:52:49,281\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.8.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-11-15 19:52:49,623\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.8.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from ray import tune\n",
    "from ray.train import Checkpoint, get_checkpoint, report, RunConfig\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = \"C:\\\\Users\\\\pc\\\\Documents\\\\repos\\\\mp-2\\\\nn\\\\nn-lab2\\\\\"\n",
    "\n",
    "constants = {\n",
    "    \"criterion\": torch.nn.CrossEntropyLoss(),\n",
    "    \"lr\": 0.0001,\n",
    "    \"n_epochs\": 40,\n",
    "    \"saving_model_path\": src_path + \"models\\\\raytune\"\n",
    "}\n",
    "config = {\n",
    "    \"batch_size\": tune.grid_search([64*64]),\n",
    "    \"patch_size\": tune.grid_search([32])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, constants):\n",
    "    # model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=False).to(device)\n",
    "    # model.classifier[6] = torch.nn.Linear(in_features=4096, out_features=4, bias=True).to(device)\n",
    "\n",
    "    criterion = constants[\"criterion\"]\n",
    "    optimizer = optim.Adam(model.parameters(), lr=constants[\"lr\"])\n",
    "    n_epochs = constants[\"n_epochs\"]\n",
    "    saving_model_path = os.path.join(constants[\"saving_model_path\"], \n",
    "                                \"checkpoint_{batch_size}_{patch_size}.model\".format(\n",
    "                                    batch_size=config['batch_size'], \n",
    "                                    patch_size=config['patch_size']))\n",
    "\n",
    "    # train_loader = WrappedDataLoader(DataLoader(train_dataset, batch_size=1, shuffle=True), to_device)\n",
    "    # valid_loader = WrappedDataLoader(DataLoader(validation_dataset, batch_size=1, shuffle=False), to_device)\n",
    "    # test_loader = WrappedDataLoader(DataLoader(test_dataset, batch_size=1, shuffle=False), to_device)\n",
    "\n",
    "    print('\\nFitting nn model')\n",
    "    start_time = time.time()\n",
    "\n",
    "    losses_arr = fit(n_epochs, model, criterion, optimizer, test_loader, valid_loader, cross_entropy_weights.to(device))\n",
    "    print(f'Fit time: {time.time() - start_time} s')\n",
    "\n",
    "    check_point = torch.load('final_model.pt', map_location=device)\n",
    "    model.load_state_dict(check_point)\n",
    "\n",
    "    test_loss, test_accuracy = evaluate(model, criterion, test_loader)\n",
    "\n",
    "    if saving_model_path is not None:\n",
    "        print('Saving model')\n",
    "        torch.save((model.state_dict(), optimizer.state_dict()), saving_model_path)\n",
    "        checkpoint = Checkpoint.from_directory(constants[\"saving_model_path\"])\n",
    "        report(\n",
    "            {\"loss\": test_loss, \"accuracy\": test_accuracy},\n",
    "            checkpoint = checkpoint\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_samples=2, gpus_per_trial=0.125):\n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_resources(\n",
    "            tune.with_parameters(train, constants=constants),\n",
    "            resources={\"cpu\": 0.25, \"gpu\": gpus_per_trial}\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"accuracy\",\n",
    "            mode=\"max\",\n",
    "            # scheduler=scheduler,\n",
    "            num_samples=num_samples,\n",
    "            # search_alg=ax_search\n",
    "        ),\n",
    "        param_space=config,\n",
    "        run_config=RunConfig(storage_path=os.path.join(src_path, \"raytune\"))\n",
    "    )\n",
    "    results = tuner.fit()\n",
    "    \n",
    "    best_result = results.get_best_result(\"accuracy\", \"max\")\n",
    "\n",
    "    return best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting nn model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1437/1437 [1:29:40<00:00,  3.74s/it]\n",
      "100%|██████████| 1149/1149 [1:13:22<00:00,  3.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:   1, loss: 3.73238, valid loss: 0.66965, valid accruacy: 81.651%\n",
      "Validation loss decreased (inf --> 0.669654). Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 25/1437 [01:27<1:22:35,  3.51s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# best = train_model(num_samples=1)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m train({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m64\u001b[39m}, constants)\n",
      "Cell \u001b[1;32mIn[24], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(config, constants)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFitting nn model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     18\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 20\u001b[0m losses_arr \u001b[38;5;241m=\u001b[39m fit(n_epochs, model, criterion, optimizer, test_loader, valid_loader, cross_entropy_weights\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFit time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m s\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     23\u001b[0m check_point \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_model.pt\u001b[39m\u001b[38;5;124m'\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\pc\\Documents\\repos\\mp-2\\nn\\nn-lab2\\train.py:89\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(epochs, model, loss_func, optimizer, train_loader, valid_loader, class_weights, patience)\u001b[0m\n\u001b[0;32m     85\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     87\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(train_loader):\n\u001b[0;32m     90\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[0;32m     91\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(patch_loss(model, loss_func, X, y, class_weights, optimizer))\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 10\u001b[0m, in \u001b[0;36mWrappedDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader):\n\u001b[0;32m     11\u001b[0m         batch_cuda \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m batch:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\pc\\Documents\\repos\\mp-2\\nn\\nn-lab2\\data.py:89\u001b[0m, in \u001b[0;36mPlantOrgansDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[1;32m---> 89\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[index][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     90\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[index][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     91\u001b[0m     size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[index][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msize\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\datasets\\arrow_dataset.py:2872\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2870\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[0;32m   2871\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem(key)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\datasets\\arrow_dataset.py:2857\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[1;34m(self, key, **kwargs)\u001b[0m\n\u001b[0;32m   2855\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m   2856\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices)\n\u001b[1;32m-> 2857\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m format_table(\n\u001b[0;32m   2858\u001b[0m     pa_subtable, key, formatter\u001b[38;5;241m=\u001b[39mformatter, format_columns\u001b[38;5;241m=\u001b[39mformat_columns, output_all_columns\u001b[38;5;241m=\u001b[39moutput_all_columns\n\u001b[0;32m   2859\u001b[0m )\n\u001b[0;32m   2860\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\datasets\\formatting\\formatting.py:639\u001b[0m, in \u001b[0;36mformat_table\u001b[1;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[0;32m    637\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39mformatter\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m formatter(pa_table, query_type\u001b[38;5;241m=\u001b[39mquery_type)\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\datasets\\formatting\\formatting.py:403\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[1;34m(self, pa_table, query_type)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable, query_type: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[RowFormat, ColumnFormat, BatchFormat]:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_row(pa_table)\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    405\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_column(pa_table)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\datasets\\formatting\\formatting.py:444\u001b[0m, in \u001b[0;36mPythonFormatter.format_row\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LazyRow(pa_table, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    443\u001b[0m row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_arrow_extractor()\u001b[38;5;241m.\u001b[39mextract_row(pa_table)\n\u001b[1;32m--> 444\u001b[0m row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_features_decoder\u001b[38;5;241m.\u001b[39mdecode_row(row)\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m row\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\datasets\\formatting\\formatting.py:222\u001b[0m, in \u001b[0;36mPythonFeaturesDecoder.decode_row\u001b[1;34m(self, row)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_row\u001b[39m(\u001b[38;5;28mself\u001b[39m, row: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;241m.\u001b[39mdecode_example(row) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;28;01melse\u001b[39;00m row\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\datasets\\features\\features.py:2039\u001b[0m, in \u001b[0;36mFeatures.decode_example\u001b[1;34m(self, example, token_per_repo_id)\u001b[0m\n\u001b[0;32m   2024\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_example\u001b[39m(\u001b[38;5;28mself\u001b[39m, example: \u001b[38;5;28mdict\u001b[39m, token_per_repo_id: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   2025\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Decode example with custom feature decoding.\u001b[39;00m\n\u001b[0;32m   2026\u001b[0m \n\u001b[0;32m   2027\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2035\u001b[0m \u001b[38;5;124;03m        `dict[str, Any]`\u001b[39;00m\n\u001b[0;32m   2036\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   2038\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m-> 2039\u001b[0m         column_name: decode_nested_example(feature, value, token_per_repo_id\u001b[38;5;241m=\u001b[39mtoken_per_repo_id)\n\u001b[0;32m   2040\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_column_requires_decoding[column_name]\n\u001b[0;32m   2041\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m value\n\u001b[0;32m   2042\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m column_name, (feature, value) \u001b[38;5;129;01min\u001b[39;00m zip_dict(\n\u001b[0;32m   2043\u001b[0m             {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m example}, example\n\u001b[0;32m   2044\u001b[0m         )\n\u001b[0;32m   2045\u001b[0m     }\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\datasets\\features\\features.py:1400\u001b[0m, in \u001b[0;36mdecode_nested_example\u001b[1;34m(schema, obj, token_per_repo_id)\u001b[0m\n\u001b[0;32m   1397\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (Audio, Image)):\n\u001b[0;32m   1398\u001b[0m     \u001b[38;5;66;03m# we pass the token to read and decode files from private repositories in streaming mode\u001b[39;00m\n\u001b[0;32m   1399\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m schema\u001b[38;5;241m.\u001b[39mdecode:\n\u001b[1;32m-> 1400\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m schema\u001b[38;5;241m.\u001b[39mdecode_example(obj, token_per_repo_id\u001b[38;5;241m=\u001b[39mtoken_per_repo_id)\n\u001b[0;32m   1401\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\datasets\\features\\image.py:188\u001b[0m, in \u001b[0;36mImage.decode_example\u001b[1;34m(self, value, token_per_repo_id)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     image \u001b[38;5;241m=\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mopen(BytesIO(bytes_))\n\u001b[1;32m--> 188\u001b[0m image\u001b[38;5;241m.\u001b[39mload()  \u001b[38;5;66;03m# to avoid \"Too many open files\" errors\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mgetexif()\u001b[38;5;241m.\u001b[39mget(PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mExifTags\u001b[38;5;241m.\u001b[39mBase\u001b[38;5;241m.\u001b[39mOrientation) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     image \u001b[38;5;241m=\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImageOps\u001b[38;5;241m.\u001b[39mexif_transpose(image)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\PIL\\ImageFile.py:291\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    290\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 291\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(b)\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# best = train_model(num_samples=1)\n",
    "result = train({\"batch_size\": 1024, \"patch_size\": 64}, constants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_10928\\4026949323.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_point = torch.load('final_model.pt', map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "check_point = torch.load('final_model.pt', map_location=device)\n",
    "model.load_state_dict(check_point)\n",
    "# test_loss, test_accuracy = evaluate(model, constants[\"criterion\"], test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for batch in valid_loader:\n",
    "    for X, y in batch:\n",
    "        predicted = model(X)\n",
    "        print(torch.argmax(predicted, dim=1).unique(return_counts=True)[0].item(), \" actual \", y.unique(return_counts=True))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39mparameters()()\n",
      "\u001b[1;31mTypeError\u001b[0m: 'generator' object is not callable"
     ]
    }
   ],
   "source": [
    "model.parameters()()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
