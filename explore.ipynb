{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddfa9eb47c284ee9ab77ab110435b1b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b251223524c43e2a85c43374f6f520e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\pc/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Using cache found in C:\\Users\\pc/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "from data import dataset, PlantOrgansDataset\n",
    "from preprocessing import preprocess_image_and_mask\n",
    "import torchvision.transforms.v2 as T\n",
    "import torch\n",
    "import numpy as np\n",
    "from alexnet import MyTransform, SlidingWindow, ExtractFeatures, get_extractor, get_feature, get_model\n",
    "from train import device, pixel_validate, patch_loss, patch_validate, evaluate, fit\n",
    "import torch.utils.data as data_utils\n",
    "from kmeans import KMeans, KNN\n",
    "from evaluate import calculate_metrics\n",
    "import torchmetrics\n",
    "from torch.utils.data import DataLoader\n",
    "from evaluate import perform_segmentation, segmentation_image, retrieve_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "commonTransform = T.Compose([\n",
    "        T.Resize(size=(2048, 2048)),\n",
    "        T.ToImage()\n",
    "        \n",
    "        # T.RandomHorizontalFlip(p=0.5),\n",
    "        # T.RandomVerticalFlip(p=0.5),\n",
    "        # T.RandomRotation(degrees=45)\n",
    "    ])\n",
    "imagesTransform = T.Compose([\n",
    "    T.ToDtype(torch.float32, scale=False),\n",
    "    # T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    MyTransform(64),\n",
    "    T.Resize((224, 224)),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "masksTransform = T.Compose([\n",
    "    T.ToDtype(torch.int8, scale=False),\n",
    "    # T.Normalize(mean=[0.0014], std=[0.0031]),\n",
    "    MyTransform(64),\n",
    "    # T.Resize((224, 224))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validation_data = dataset['train'].train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = PlantOrgansDataset(train_validation_data['train'], commonTransform, imagesTransform, masksTransform)\n",
    "validation_dataset = PlantOrgansDataset(train_validation_data['test'], commonTransform, imagesTransform, masksTransform)\n",
    "test_dataset = PlantOrgansDataset(dataset['validation'], commonTransform, imagesTransform, masksTransform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_weights = torch.tensor([\n",
    "        4.8033e-04,\n",
    "        6.4129e-03,\n",
    "        3.9272e-03,\n",
    "        9.7140e-01,\n",
    "        1.7778e-02], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset:  4596\n",
      "validation_dataset:  1149\n",
      "test_dataset:  1437\n"
     ]
    }
   ],
   "source": [
    "print(\"train_dataset: \", len(train_dataset))\n",
    "print(\"validation_dataset: \", len(validation_dataset))\n",
    "print(\"test_dataset: \", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedDataLoader:\n",
    "    def __init__(self, loader, func):\n",
    "        self.loader = loader\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.loader)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in iter(self.loader):\n",
    "            batch_cuda = []\n",
    "            for X, y in batch:\n",
    "                batch_cuda.append(self.func(X, y))\n",
    "            yield batch_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(X: torch.Tensor, y: torch.Tensor):\n",
    "    return X.to(device, dtype=torch.float32), y.to(device, dtype=torch.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    batchs_amount = len(batch)\n",
    "    current_images = []\n",
    "    current_masks = []\n",
    "    current_length = 0\n",
    "    i = 0\n",
    "    while i < batchs_amount or current_length >= batch_size:\n",
    "        if current_length == batch_size:\n",
    "            if len(current_images) == 1:\n",
    "                result_images = current_images[0]\n",
    "                result_masks = current_masks[0]\n",
    "            else:\n",
    "                result_images = torch.concatenate(current_images)\n",
    "                result_masks = torch.concatenate(current_masks)\n",
    "            current_images = []\n",
    "            current_masks = []\n",
    "            current_length = 0\n",
    "            yield result_images, result_masks\n",
    "        elif current_length > batch_size:\n",
    "            concatenated_images = torch.concatenate(current_images)\n",
    "            concatenated_masks = torch.concatenate(current_masks)\n",
    "            images_split = torch.split(concatenated_images, batch_size, dim=0)\n",
    "            masks_split = torch.split(concatenated_masks, batch_size, dim=0)\n",
    "            current_images = [images_split[len(images_split) - 1]]\n",
    "            current_masks = [masks_split[len(masks_split) - 1]]\n",
    "            current_length = len(current_images[0])\n",
    "            for j in range(len(images_split) - 1):\n",
    "                yield images_split[j], masks_split[j]\n",
    "        else:  \n",
    "            images, masks = batch[i]\n",
    "            i += 1\n",
    "            current_length += len(images)\n",
    "            current_images.append(images)\n",
    "            current_masks.append(masks)\n",
    "    if current_length > 0:\n",
    "        concatenated_images = torch.concatenate(current_images)\n",
    "        concatenated_masks = torch.concatenate(current_masks)\n",
    "        yield concatenated_images, concatenated_masks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = WrappedDataLoader(\n",
    "    DataLoader(train_dataset, batch_size=1, shuffle=False, collate_fn=custom_collate_fn, \n",
    "               pin_memory=True, pin_memory_device=[device]), to_device)\n",
    "valid_loader = WrappedDataLoader(\n",
    "    DataLoader(validation_dataset, batch_size=1, shuffle=False, collate_fn=custom_collate_fn,\n",
    "               pin_memory=False, pin_memory_device=[device]), to_device)\n",
    "test_loader = WrappedDataLoader(\n",
    "    DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=custom_collate_fn,\n",
    "               pin_memory=False, pin_memory_device=[device]), to_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 01:06:48,040\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.8.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-11-17 01:06:48,347\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.8.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from ray import tune\n",
    "from ray.train import Checkpoint, get_checkpoint, report, RunConfig\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = \"C:\\\\Users\\\\pc\\\\Documents\\\\repos\\\\mp-2\\\\nn\\\\nn-lab2\\\\\"\n",
    "\n",
    "constants = {\n",
    "    \"criterion\": torch.nn.CrossEntropyLoss(),\n",
    "    \"lr\": 0.0001,\n",
    "    \"n_epochs\": 40,\n",
    "    \"saving_model_path\": src_path + \"models\\\\raytune\"\n",
    "}\n",
    "config = {\n",
    "    \"batch_size\": tune.grid_search([64*64]),\n",
    "    \"patch_size\": tune.grid_search([32])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "image_to_tensor = T.Compose([\n",
    "    # T.ToImage(),\n",
    "    T.ToDtype(dtype=torch.float32, scale=True),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    T.Resize(size=(2048, 2048)),\n",
    "])\n",
    "mask_to_tensor = T.Compose([\n",
    "    T.ToImage(),\n",
    "    T.ToDtype(dtype=torch.float32, scale=False)\n",
    "])\n",
    "mask_of_uniform_size = T.Compose([\n",
    "    T.Resize((2048, 2048), interpolation=T.InterpolationMode.NEAREST_EXACT),\n",
    "    mask_to_tensor,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = train_validation_data['train'][1]['image']\n",
    "X = image_to_tensor(image).unsqueeze(0).to(device)\n",
    "y = mask_of_uniform_size(train_validation_data['train'][1]['label']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T.Resize((224, 224))(train_validation_data['train'][1]['image'])\n",
    "# T.Resize((224, 224))(mask_to_image(train_validation_data['train'][1]['label']))\n",
    "\n",
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "# get_graph_node_names(model)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# import torch\n",
    "# import gc\n",
    "# objects = []\n",
    "# for obj in gc.get_objects():\n",
    "#     try:\n",
    "#         if (torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data))) and obj.is_cuda:\n",
    "#             objects.append((type(obj), obj.size(), obj.numel()))\n",
    "#     except:\n",
    "#         pass\n",
    "\n",
    "# sorted_by_size = sorted(objects, key=lambda tup: tup[2], reverse=True)\n",
    "# sorted_by_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_classes = perform_segmentation(X, y.unsqueeze(0))\n",
    "# segmentation_image(predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X, y\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segmentation_image(mask_to_tensor(train_validation_data['train'][1]['label'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = train_validation_data['train'][1]['image']\n",
    "# X = image_to_tensor(image).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tqdm\n",
    "# for i in tqdm.tqdm(range(100)):\n",
    "#     image = train_validation_data['train'][i]['image']\n",
    "#     features = retrieve_features(image_to_tensor(image).unsqueeze(0).to(device))\n",
    "#     save_features(i, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import lz4\n",
    "\n",
    "def read_features(index, src_path, layer_name=\"classifier.0\", sliding_window_size=32, \n",
    "                         sliding_window_step=11):\n",
    "    file_name = os.path.join(src_path, \"features\", \n",
    "                            \"train_{layer_name}_{window_size}_{window_step}_{i}.lz4\"\n",
    "                              .format(layer_name=layer_name, window_size=sliding_window_size, window_step=sliding_window_step, i=index)\n",
    "                            )\n",
    "    with lz4.frame.open(file_name, mode=\"rb\") as f:\n",
    "      features_shape = pickle.load(f)\n",
    "      features = pickle.load(f)\n",
    "      return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn = KNN(features.view(features.size(0) * features.size(1), -1), ground_truth_patches, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    }
   ],
   "source": [
    "knn_features = []\n",
    "for i in range(5):\n",
    "    knn_features.append(read_features(i, src_path).to(\"cpu\"))\n",
    "    knn_features[i] = knn_features[i].view(knn_features[i].size(0) * knn_features[i].size(1), -1)\n",
    "torch.cuda.empty_cache()\n",
    "merged_knn_features = torch.cat(knn_features, dim=0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truth_for_indices(indices):\n",
    "    ground_truths = []\n",
    "    for i in indices:\n",
    "        ground_truth = mask_of_uniform_size(train_validation_data['train'][i]['label']).to(\"cpu\")\n",
    "        mask_patches = SlidingWindow(32, 11)(ground_truth.unsqueeze(0))\n",
    "        mask_patches_ = mask_patches.view(mask_patches.size(0), mask_patches.size(1), -1)\n",
    "        ground_truths.append(torch.mode(mask_patches_, dim=2).values.to(dtype=torch.int8).view(-1))\n",
    "    merged_ground_truths = torch.cat(ground_truths, dim=0).to(device)\n",
    "    del ground_truths, ground_truth, mask_patches, mask_patches_\n",
    "    return merged_ground_truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truths = get_ground_truth_for_indices(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0', dtype=torch.int8)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNN(merged_knn_features, ground_truths, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = read_features(11, src_path).to(\"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([187, 187, 9216])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m187\u001b[39m, \u001b[38;5;241m32\u001b[39m):\n\u001b[0;32m      3\u001b[0m     i_upper_limit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m31\u001b[39m, \u001b[38;5;241m187\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     predicted_classes \u001b[38;5;241m=\u001b[39m knn\u001b[38;5;241m.\u001b[39mpredict(test_features[i : i_upper_limit]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m9216\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64))\n\u001b[0;32m      5\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m      6\u001b[0m     predicted_mask[i : i_upper_limit] \u001b[38;5;241m=\u001b[39m predicted_classes\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m187\u001b[39m, \u001b[38;5;241m9216\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pc\\Documents\\repos\\mp-2\\nn\\nn-lab2\\kmeans.py:89\u001b[0m, in \u001b[0;36mKNN.predict\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     86\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m wasn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt trained. Need to execute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.train() first\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 89\u001b[0m dist \u001b[38;5;241m=\u001b[39m distance_matrix(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_pts, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp)\n\u001b[0;32m     91\u001b[0m knn \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mtopk(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk, largest\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     92\u001b[0m votes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_label[knn\u001b[38;5;241m.\u001b[39mindices]\n",
      "File \u001b[1;32mc:\\Users\\pc\\Documents\\repos\\mp-2\\nn\\nn-lab2\\kmeans.py:20\u001b[0m, in \u001b[0;36mdistance_matrix\u001b[1;34m(x, y, p)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Compute distances in a memory-efficient way\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(m):  \u001b[38;5;66;03m# Loop over y vectors\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     diff \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msub(x, y[i])   \u001b[38;5;66;03m# Broadcast only over x, avoiding full tensor expansion\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     dist[:, i] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mvector_norm(diff, p, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m__version__ \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1.7.0\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mpow(diff, p)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mp)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dist\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predicted_mask = torch.zeros((187, 187))\n",
    "for i in range(0, 187, 24):\n",
    "    i_upper_limit = min(i+23, 187)\n",
    "    predicted_classes = knn.predict(test_features[i : i_upper_limit].view(-1, 9216).to(device, dtype=torch.int64))\n",
    "    torch.cuda.empty_cache()\n",
    "    predicted_mask[i : i_upper_limit] = predicted_classes.view(-1, 187, 9216).to(\"cpu\")\n",
    "    print(i)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
