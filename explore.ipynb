{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c2bb103f9e416ea4dd21c51eccbe65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4813ff8ec4b94aa398739c86c16db85a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from data import dataset, PlantOrgansDataset\n",
    "from preprocessing import preprocess_image_and_mask\n",
    "import torchvision.transforms.v2 as T\n",
    "import torch\n",
    "import numpy as np\n",
    "from alexnet import patch_index_to_position, image_to_patches, MyTransform\n",
    "from train import device, pixel_validate, patch_loss, patch_validate, evaluate, fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "commonTransform = T.Compose([\n",
    "        T.Resize(size=(2048, 2048)),\n",
    "        T.ToImage()\n",
    "        \n",
    "        # T.RandomHorizontalFlip(p=0.5),\n",
    "        # T.RandomVerticalFlip(p=0.5),\n",
    "        # T.RandomRotation(degrees=45)\n",
    "    ])\n",
    "imagesTransform = T.Compose([\n",
    "    T.ToDtype(torch.float32, scale=False),\n",
    "    # T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    MyTransform(32),\n",
    "    T.Resize((224, 224)),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "masksTransform = T.Compose([\n",
    "    T.ToDtype(torch.int8, scale=False),\n",
    "    # T.Normalize(mean=[0.0014], std=[0.0031]),\n",
    "    MyTransform(32),\n",
    "    # T.Resize((224, 224))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validation_data = dataset['train'].train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = PlantOrgansDataset(train_validation_data['train'], commonTransform, imagesTransform, masksTransform)\n",
    "validation_dataset = PlantOrgansDataset(train_validation_data['test'], commonTransform, imagesTransform, masksTransform)\n",
    "test_dataset = PlantOrgansDataset(dataset['validation'], commonTransform, imagesTransform, masksTransform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4, dtype=torch.int8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = T.Compose([\n",
    "    T.ToImage(),\n",
    "    T.ToDtype(dtype=torch.int8, scale=False)\n",
    "])\n",
    "tensor = tr(train_validation_data['train'][0]['label'])\n",
    "tensor.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_weights = torch.tensor([\n",
    "        4.8033e-04,\n",
    "        6.4129e-03,\n",
    "        3.9272e-03,\n",
    "        9.7140e-01,\n",
    "        1.7778e-02], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset:  4596\n",
      "validation_dataset:  1149\n",
      "test_dataset:  1437\n"
     ]
    }
   ],
   "source": [
    "print(\"train_dataset: \", len(train_dataset))\n",
    "print(\"validation_dataset: \", len(validation_dataset))\n",
    "print(\"test_dataset: \", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedDataLoader:\n",
    "    def __init__(self, loader, func):\n",
    "        self.loader = loader\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.loader)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in iter(self.loader):\n",
    "            batch_cuda = []\n",
    "            for X, y in batch:\n",
    "                batch_cuda.append(self.func(X, y))\n",
    "            yield batch_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(X: torch.Tensor, y: torch.Tensor):\n",
    "    return X.to(device, dtype=torch.float32), y.to(device, dtype=torch.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    batchs_amount = len(batch)\n",
    "    current_images = []\n",
    "    current_masks = []\n",
    "    current_length = 0\n",
    "    i = 0\n",
    "    while i < batchs_amount or current_length >= batch_size:\n",
    "        if current_length == batch_size:\n",
    "            if len(current_images) == 1:\n",
    "                result_images = current_images[0]\n",
    "                result_masks = current_masks[0]\n",
    "            else:\n",
    "                result_images = torch.concatenate(current_images)\n",
    "                result_masks = torch.concatenate(current_masks)\n",
    "            current_images = []\n",
    "            current_masks = []\n",
    "            current_length = 0\n",
    "            yield result_images, result_masks\n",
    "        elif current_length > batch_size:\n",
    "            concatenated_images = torch.concatenate(current_images)\n",
    "            concatenated_masks = torch.concatenate(current_masks)\n",
    "            images_split = torch.split(concatenated_images, batch_size, dim=0)\n",
    "            masks_split = torch.split(concatenated_masks, batch_size, dim=0)\n",
    "            current_images = [images_split[len(images_split) - 1]]\n",
    "            current_masks = [masks_split[len(masks_split) - 1]]\n",
    "            current_length = len(current_images[0])\n",
    "            for j in range(len(images_split) - 1):\n",
    "                yield images_split[j], masks_split[j]\n",
    "        else:  \n",
    "            images, masks = batch[i]\n",
    "            i += 1\n",
    "            current_length += len(images)\n",
    "            current_images.append(images)\n",
    "            current_masks.append(masks)\n",
    "    if current_length > 0:\n",
    "        concatenated_images = torch.concatenate(current_images)\n",
    "        concatenated_masks = torch.concatenate(current_masks)\n",
    "        yield concatenated_images, concatenated_masks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = WrappedDataLoader(\n",
    "    DataLoader(train_dataset, batch_size=1, shuffle=False, collate_fn=custom_collate_fn, \n",
    "               pin_memory=False, pin_memory_device=[device]), to_device)\n",
    "valid_loader = WrappedDataLoader(\n",
    "    DataLoader(validation_dataset, batch_size=1, shuffle=False, collate_fn=custom_collate_fn,\n",
    "               pin_memory=False, pin_memory_device=[device]), to_device)\n",
    "test_loader = WrappedDataLoader(\n",
    "    DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=custom_collate_fn,\n",
    "               pin_memory=False, pin_memory_device=[device]), to_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\pc/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=False).to(device)\n",
    "\n",
    "model.classifier[6] = torch.nn.Linear(in_features=4096, out_features=5, bias=True).to(device)\n",
    "\n",
    "# model = AlexNet([11, 5, 3, 3, 3], [96, 256, 384, 384, 256], [0, 2, 1, 1, 1], [4096, 4096], 4, 224).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "to_image = T.ToPILImage()\n",
    "mask_to_image = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.ToDtype(torch.float16),\n",
    "    T.Normalize(mean=[0.0014], std=[0.0031]),\n",
    "    T.ToPILImage(),\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:594: UserWarning: pin memory device is set and pin_memory flag is not used then device pinned memory won't be usedplease set pin_memory to true, if you need to use the device pin memory\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 3, 224, 224])\n",
      "2.473323345184326 8\n",
      "torch.Size([512, 3, 224, 224])\n",
      "2.3403139114379883 21\n",
      "torch.Size([512, 3, 224, 224])\n",
      "2.603787899017334 8\n",
      "torch.Size([512, 3, 224, 224])\n",
      "2.096975803375244 33\n",
      "torch.Size([512, 3, 224, 224])\n",
      "2.2859718799591064 18\n",
      "torch.Size([512, 3, 224, 224])\n",
      "2.6517460346221924 12\n",
      "torch.Size([512, 3, 224, 224])\n",
      "3.3301150798797607 5\n",
      "torch.Size([512, 3, 224, 224])\n",
      "3.9067533016204834 6\n"
     ]
    }
   ],
   "source": [
    "# for batch in valid_loader:\n",
    "#     # print(len(batch))\n",
    "#     for X, y in batch:\n",
    "#         print(X.shape)\n",
    "#         predicted = model(X)\n",
    "#         loss_, correct, _ = patch_validate(model, torch.nn.CrossEntropyLoss(weight=cross_entropy_weights), X, y)\n",
    "#         print(loss_, correct)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts = torch.zeros(size=(5, 1), dtype=torch.int64).to(device)\n",
    "# for batch in valid_loader:\n",
    "#     for X, y in batch:\n",
    "#         current_counts = y.unique(return_counts=True)\n",
    "#         for i in current_counts[0]:\n",
    "#             counts[current_counts[0][i]] += current_counts[1][i]\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts_sum = torch.sum(counts).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = torch.div(counts_sum, torch.multiply(counts, 2549.76513671875))\n",
    "\n",
    "# print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 21:38:11,935\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.8.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-11-13 21:38:12,519\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.8.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from ray import tune\n",
    "from ray.train import Checkpoint, get_checkpoint, report, RunConfig\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = \"C:\\\\Users\\\\pc\\\\Documents\\\\repos\\\\mp-2\\\\nn\\\\nn-lab2\\\\\"\n",
    "\n",
    "constants = {\n",
    "    \"criterion\": torch.nn.CrossEntropyLoss(weight=cross_entropy_weights),\n",
    "    \"lr\": 0.001,\n",
    "    \"n_epochs\": 40,\n",
    "    \"saving_model_path\": src_path + \"models\\\\raytune\"\n",
    "}\n",
    "config = {\n",
    "    \"batch_size\": tune.grid_search([64*64]),\n",
    "    \"patch_size\": tune.grid_search([32])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, constants):\n",
    "    # model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=False).to(device)\n",
    "    # model.classifier[6] = torch.nn.Linear(in_features=4096, out_features=4, bias=True).to(device)\n",
    "\n",
    "    criterion = constants[\"criterion\"]\n",
    "    optimizer = optim.Adam(model.parameters(), lr=constants[\"lr\"])\n",
    "    n_epochs = constants[\"n_epochs\"]\n",
    "    saving_model_path = os.path.join(constants[\"saving_model_path\"], \n",
    "                                \"checkpoint_{batch_size}_{patch_size}.model\".format(\n",
    "                                    batch_size=config['batch_size'], \n",
    "                                    patch_size=config['patch_size']))\n",
    "\n",
    "    # train_loader = WrappedDataLoader(DataLoader(train_dataset, batch_size=1, shuffle=True), to_device)\n",
    "    # valid_loader = WrappedDataLoader(DataLoader(validation_dataset, batch_size=1, shuffle=False), to_device)\n",
    "    # test_loader = WrappedDataLoader(DataLoader(test_dataset, batch_size=1, shuffle=False), to_device)\n",
    "\n",
    "    print('\\nFitting nn model')\n",
    "    start_time = time.time()\n",
    "\n",
    "    losses_arr = fit(n_epochs, model, criterion, optimizer, train_loader, valid_loader)\n",
    "    print(f'Fit time: {time.time() - start_time} s')\n",
    "\n",
    "    check_point = torch.load('final_model.pt', map_location=device)\n",
    "    model.load_state_dict(check_point)\n",
    "\n",
    "    test_loss, test_accuracy = evaluate(model, criterion, test_loader)\n",
    "\n",
    "    if saving_model_path is not None:\n",
    "        print('Saving model')\n",
    "        torch.save((model.state_dict(), optimizer.state_dict()), saving_model_path)\n",
    "        checkpoint = Checkpoint.from_directory(constants[\"saving_model_path\"])\n",
    "        report(\n",
    "            {\"loss\": test_loss, \"accuracy\": test_accuracy},\n",
    "            checkpoint = checkpoint\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_samples=2, gpus_per_trial=0.125):\n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_resources(\n",
    "            tune.with_parameters(train, constants=constants),\n",
    "            resources={\"cpu\": 0.25, \"gpu\": gpus_per_trial}\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"accuracy\",\n",
    "            mode=\"max\",\n",
    "            # scheduler=scheduler,\n",
    "            num_samples=num_samples,\n",
    "            # search_alg=ax_search\n",
    "        ),\n",
    "        param_space=config,\n",
    "        run_config=RunConfig(storage_path=os.path.join(src_path, \"raytune\"))\n",
    "    )\n",
    "    results = tuner.fit()\n",
    "    \n",
    "    best_result = results.get_best_result(\"accuracy\", \"max\")\n",
    "\n",
    "    return best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting nn model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]c:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:594: UserWarning: pin memory device is set and pin_memory flag is not used then device pinned memory won't be usedplease set pin_memory to true, if you need to use the device pin memory\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# best = train_model(num_samples=1)\n",
    "result = train({\"batch_size\": 64*64, \"patch_size\": 32}, constants)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
